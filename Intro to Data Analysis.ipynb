{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5b0500-2397-4dc3-bc70-e54e5f24aaaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Intro to Data Analysis with Python\n",
    "\n",
    "In this notebook, we will show you how to solve a data analysis problem end-to-end.\n",
    "\n",
    "The steps and decisions I took may not be the best or the fastest - the goal is to give you an **overview** of what steps might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9fcf5d",
   "metadata": {
    "heading_collapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7b2cf4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1.1. **Comments**\n",
    "\n",
    "First, how to write comments. Feel free to annotate a lot, so that you remember what you did and why :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f81e834",
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is a comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1790ca3",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1.2. Syntax\n",
    "\n",
    "Below are some examples to get a feel for Python if you've never seen it before. There are (for our level) no types for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d378be",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "\n",
    "x = 5\n",
    "y = 0.25\n",
    "z = \"some text\"\n",
    "\n",
    "# lists\n",
    "a = [1, 2, 3]\n",
    "b = [x, y, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82242c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(x + y)\n",
    "print(x * z)\n",
    "# print(y * z)    # this does not work\n",
    "\n",
    "print(b)          # lists can have any objects as elements\n",
    "print(a * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc75fbd",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Since Python doesn't use brackets to divide namespaces, the spacing is very important. For example, **all the code inside a loop or an if-else must have the same indent!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c3bd85",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loops\n",
    "\n",
    "for i in range(len(a)): # range(n) yields all values in the interval [0, n-1]\n",
    "    a[i] += 2           # select element with a[.]\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebdb95",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Conditionals \n",
    "\n",
    "def is_odd(x):\n",
    "    if x % 2 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "is_odd(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce91499",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Python also has `while`-loops and cool things like *comprehensions*, but let's leave that for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da80207",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1.3. numpy\n",
    "\n",
    "One of the most important libraries when working with numerical data is `numpy`. It provides support for many numerical operations on scalars, vectors, matrices and fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fecb90",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np   # importing numpy with the alias np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acb1f4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n = np.array([1, 2, 3])                   # initialize array with a list\n",
    "m = np.array([[1, 2, 3], [4, 5, 6]])      # initialize 2D array with list of lists -> matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c842f9fb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620ab099",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n.shape     # very important attribute, tells us the form of the vector or array\n",
    " \n",
    "# (3,) means that a is a column (standing) vector with 3 elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a0fe8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n.sum()           # no arguments -> sum of all elements in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f9979",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7534af1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m.shape   # 2 rows, 3 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900fa0a8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m.sum(axis=0)     # axis=0 means per column, axis=1 means per row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d527b0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc649416",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb83a6",
   "metadata": {},
   "source": [
    "Two libraries will help us analyse our data - `pandas` and `matplotlib`.\n",
    "\n",
    "We use `pandas` to read in tabular data from local files, as well as to clean and transform the data. It also offers some visualization functions, but the freedom there is limited.\n",
    "`matplotlib` is a rich visualization library.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1fc51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np       # vectors and matrices + functions\n",
    "import pandas as pd      # DataFrame data structure + functions\n",
    "import matplotlib.pyplot as plt   # visualization\n",
    "import seaborn as sns             # visualization\n",
    "\n",
    "# show the visualizations in-line\n",
    "%matplotlib inline       \n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff8a8c9-282f-4809-a6e8-ca0e4b28942d",
   "metadata": {},
   "source": [
    "_____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8b2fd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data\n",
    "\n",
    "The dataset is a simplified version of the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/index.php)'s [Statlog (German Credit Data) Data Set](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29). \n",
    "\n",
    "Modified version downloaded from [Kaggle](https://www.kaggle.com/datasets/kabure/german-credit-data-with-risk). [License](https://creativecommons.org/publicdomain/zero/1.0/).\n",
    "\n",
    "**The data set consists of customers that have taken out a credit. Each credit is classified as low or high risk according to the set of attributes:**\n",
    "\n",
    "Feature ID| Feature Name |Description | Type | Values\n",
    "--|-----|-----|----|---\n",
    "1 | Age | person's age | numeric | age of the bank customer\n",
    "2 | Sex | person's sex | text | \"male\" or \"female\"\n",
    "3 | Job | type of employment | numeric | 0 - unskilled and non-resident, 1 - unskilled and resident, 2 - skilled, 3 - highly skilled\n",
    "4 | Housing | type of housing | text | \"own\", \"rent\", or \"free\"\n",
    "5 | Saving accounts | savings account balance category | text | \"little\", \"moderate\", \"quite rich\", \"rich\"\n",
    "6 | Checking account | checking account balance category | text | \"little\", \"moderate\", \"rich\"\n",
    "7 | Credit amount | credit amount | numeric | credit amount in DM (Deutsche Mark)\n",
    "8 | Duration | credit duration | numeric | duration of the credit in months - 4 to 72 months\n",
    "9 | Purpose | credit purpose | text | \"car\", \"furniture/equipment\", \"radio/TV\", \"domestic appliances\", \"repairs\", \"education\", \"business\", \"vacation/others\"\n",
    "\n",
    "\n",
    "**The target variable is `Risk`:**\n",
    "\n",
    "Feature ID| Feature Name |Description | Type | Values\n",
    "--|-----|-----|----|---\n",
    "10 | Risk | credit risk | text | \"low\" risk or \"high\" risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774fcbc-3e5c-41d0-885e-441efa6da5cd",
   "metadata": {},
   "source": [
    "_________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410184c0-0c57-498e-9603-9547d0e00178",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8ff23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Data/german_credit_data.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf485b65",
   "metadata": {
    "tags": []
   },
   "source": [
    "### General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first n lines (no number gives first 5)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e401933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a25d2",
   "metadata": {},
   "source": [
    "A lot of info here: \n",
    "\n",
    "- 1000 entries/people\n",
    "- 9 features (Age-Purpose)\n",
    "- Missing values in the features `Saving accounts` and `Checking account`\n",
    "- Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection - column\n",
    "\n",
    "data['Purpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0dd704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection - row\n",
    "\n",
    "data.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection - condition\n",
    "\n",
    "data[data['Credit amount'] < 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32990b5e-3441-4851-8172-61ea453076c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# statistical functions\n",
    "\n",
    "data['Age'].mean()   #  .max()   .min()   .std() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1af82-ce36-44c2-b847-8d640f64e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique values and counts\n",
    "\n",
    "data['Purpose'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea84294",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b63f452",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Exploratory data analysis (EDA)\n",
    "\n",
    "My favourite way to get an overview of the features is to visualize how each is distributed. Of course, there are different types of features, numerical, categorical, text etc. For this data set, we need to take only two cases into account - numerical features (integers/floats) and categorical features (small number of distinct values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb53282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distributions\n",
    "\n",
    "# Set the size of the entire plot (I set this by trial and error :)\n",
    "fig = plt.figure(figsize=(20, 7))  \n",
    "\n",
    "# Go through all the columns (there is 10 of them with the target)\n",
    "for i, col in enumerate(data.columns):\n",
    "    \n",
    "    sp = plt.subplot(2, 5, i+1)  # 4 x 3 is the grid to place the plots in\n",
    "    \n",
    "    if len(data[col].value_counts()) > 10:    # If the column has more than 10 distinct values, we\n",
    "                          # can assume the column is not categorical\n",
    "        sns.histplot(x=col, data=data, kde=True)    # Nice histogram\n",
    "        \n",
    "    else:    # Categorical data\n",
    "        sns.countplot(x=col, data=data)   # Count plot\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.3, hspace=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c77b4a",
   "metadata": {},
   "source": [
    "**Some things we notice:**\n",
    "\n",
    "- Most people in the dataset are 20-40 years old\n",
    "- The 'Job' column has four values. Looking into the [dataset description](https://www.kaggle.com/datasets/uciml/german-credit), we can see that the meanings are as follows:\n",
    "\n",
    "| Code | Job type |\n",
    "| --- | --- |\n",
    "| 0 | unskilled and non-resident |\n",
    "| 1 | unskilled and resident |\n",
    "| 2 | skilled |\n",
    "| 3 | highly skilled |\n",
    "\n",
    "- The credit amount was usually 2000-3000 DM\n",
    "- The credit duration was usually 60 months at most, usually 12 or 24 months\n",
    "- The target variable `Risk` is unbalanced - 70% of the credits were low risk\n",
    "\n",
    "---------\n",
    "\n",
    "Another useful function is `.describe()`, which shows us important summary statistics of the features. We saw some of them in the visualizations above. `.describe()` can be used on the entire `DataFrame`, but also on chosen columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9eadcb",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Let's try to answer some data science questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d65f75",
   "metadata": {},
   "source": [
    "#### Do people with highly skilled jobs take out larger loans?\n",
    "\n",
    "We can use a boxplot to answer this question. The center line of the box represents the mean credit amount of all people with a particular job level. The box shows the quartiles, and the whiskers going up and down - the entirety of the distribution's support. The points above the whiskers are considered to be outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360feef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"Job\", y=\"Credit amount\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2f6993",
   "metadata": {},
   "source": [
    "Yes, apparently. The loans taken out by people with job level 3 are 2000 DM higher on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965a6ef",
   "metadata": {},
   "source": [
    "#### Is the credit's purpose indicative of the risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370dcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 7)) \n",
    "sns.boxplot(x=\"Purpose\", y=\"Credit amount\", data=data, hue=\"Risk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68d85aa",
   "metadata": {},
   "source": [
    "At least when taking out a loan for the `vacation/others` purpose, the credit amount is very clearly indicative of how risky the loan is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef507019",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Before we dive into the machine learning part of this notebook, we have some cleaning to do. Machine learning algorithms have problems when dealing with missing values.\n",
    "\n",
    "**How many `NaN` values are there in the data?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94bf791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd9b78-418e-4a5a-958c-7538daee3bbd",
   "metadata": {},
   "source": [
    "There are missing values in the `Saving accounts` and `Credit account` features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8173b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Missing values\n",
    "\n",
    "There are several ways to handle missing values:\n",
    "- Imputation - Replacing the missing values with a real value (e.g. mean or median of the feature)\n",
    "- Deleting entries with missing values\n",
    "- Deleting columns/features with missing values\n",
    "\n",
    "The correct way to handle missing data is not universal. For example, if the number of missing values is too high, imputation will add a lot of noise, which can make model learning very difficult.\n",
    "\n",
    "If we have a small data set, deleting entries means we would have even fewer data points to train on.\n",
    "\n",
    "If all features in a data set have missing values, it doesn't make sense to delete those features.\n",
    "\n",
    "In our case, however, the missing values are in only two of the **categorical** columns - `Saving accounts` and `Checking account`. The missing values must then correspond to cases where is was unknown whether the person had such an account, or how much money was in it.\n",
    "\n",
    "Let's check how well these features correspond to the target variable `Risk`. We will treat the `NaN` values as a separate category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ee5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=data.fillna('unknown'), y=\"Saving accounts\", hue=\"Risk\", multiple=\"stack\", shrink=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17699942",
   "metadata": {},
   "source": [
    "Then, `Checking account`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054df2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(data=data.fillna('unknown'), y=\"Checking account\", hue=\"Risk\", multiple=\"stack\", shrink=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260b3a1",
   "metadata": {},
   "source": [
    "There are many more examples of low risk than high risk when the value of the two features is `unknown`. So this category might help with the prediction of the risk. However, in the real world, this type of missing data is not random and can make our model very biased - for example, the `unknown` category's distribution most closely matches the distribution of the `rich` categories.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92e6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.fillna('unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbef1cf-c32b-441e-90e3-026b60b6a4f3",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beada07",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Convert to numerical\n",
    "\n",
    "Since most machine learning algorithms require numerical data, let's use indicator variables for each category. We need to convert all categorical features (their values are strings) to numbers. The categorical features have the dtype `object`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8ccaf-ec73-489e-b5a8-30e09cc9ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e25f4f-0107-4c15-9e44-dbb24d8e0747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5ef98d-1bbb-4d30-8767-8c519ec1af29",
   "metadata": {},
   "source": [
    "We can convert these text features to numerical by using **indicator variables**.\n",
    "\n",
    "The `pandas` function `.get_dummies()` does just that, expanding $N$ categories into $N$ indicator (binary) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac895e64-27e7-4c54-b3c5-81be2c6aa8c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_features = data.select_dtypes(include='object').columns\n",
    "\n",
    "print(categorical_features)\n",
    "\n",
    "num_data = pd.get_dummies(data, columns=categorical_features)\n",
    "num_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba3c1a-933f-47f7-8c34-991e39cbc409",
   "metadata": {},
   "source": [
    "We now have 28 columns instead of 10, but we can delete some of them in the next step.\n",
    "\n",
    "Let's make sure they're all numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ffbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b3d7d3-1404-4590-9bfb-9a0dca851eb5",
   "metadata": {},
   "source": [
    "_________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1cf7ce-c026-4f54-b866-926040514b87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Correlation\n",
    "\n",
    "For the same numerical features, we can also compute the correlation.\n",
    "\n",
    "The correlation coefficient is a value between -1 and 1. \n",
    "- A coefficient of 0 means that the two variables are not correlated, that is, we can't draw conclusions about one variable if we know the other.\n",
    "- Coefficients >0 denote a positive correlation, meaning that an increase in one variable is connected to an increase in the other variable. A value of 1 means the variables are exactly equal.\n",
    "- Coefficients <0 denote a negative correlation, meaning that an increase in one variable is connected to a decrease in the other variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = num_data.corr()\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.heatmap(correlation.round(decimals=2), annot=True, vmax=1, vmin=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0576135-19ca-458e-970c-00a874e242d4",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "- The binary indicator variables for `Sex` and `Risk` have perfect anti-correlation, so we can drop one of each, for example `Sex_male` and `Risk_low`.\n",
    "- There is a relatively high positive correlation between the variables `Credit amount` and `Duration` (0.62). Of course, larger credit amounts usually have longer credit durations, so no surprise there.\n",
    "- The variables with highest correlation with the target variable are `Checking_account_unknown` (indicates low risk) and `Checking_account_little` (indicates high risk). The features `Credit amount`, `Saving accounts_little` and `Duration` are also related to the `Risk` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693eb776-e251-49cc-b044-c01b4cb665b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = num_data.drop(['Sex_male', 'Risk_low'], axis=1)\n",
    "num_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037579d",
   "metadata": {},
   "source": [
    "We now have the `Risk_high` variable with 1 meaning high risk and 0 low risk. This is more intuitive and having 'high' be the positive class (index 1) makes evaluation easier, since we usually want to predict the high risk credits anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354b2a87-2e19-4d0c-9f3b-e6518964c497",
   "metadata": {},
   "source": [
    "__________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd82cf54",
   "metadata": {},
   "source": [
    "### Train and test data\n",
    "\n",
    "How do we go about using machine learning to predict whether a loan is low or high risk? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f7721a",
   "metadata": {},
   "source": [
    "First, we need to split our data in two separate datesets - a training data set with known labels/risk, and a test data set where we test the learned knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ea4416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = num_data.Risk_high\n",
    "features = num_data.drop(\"Risk_high\", axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels,  random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d09bd-e90d-4cb7-a516-bb063a6fab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62338c83-54e4-4a88-94bf-cd321d519b91",
   "metadata": {},
   "source": [
    "We have 750 training samples and 250 test samples.\n",
    "\n",
    "----------------------\n",
    "\n",
    "Next, we need a model. The most important question we need to answer is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a23698",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### What are we trying to predict?   \n",
    "\n",
    "The `Risk_high` variable, which is binary (two categories).\n",
    "\n",
    "The problem to solve is called [binary classification](https://en.wikipedia.org/wiki/Binary_classification).\n",
    "There are many algorithms that can solve binary classification problems. We will look at two example models - [decision tree](https://en.wikipedia.org/wiki/Decision_tree_learning) and [logistic regression](https://kambria.io/blog/logistic-regression-for-machine-learning/#:~:text=What%20Is%20Logistic%20Regression%3F,either%20a%200%20or%201.). Let's import the algorithms and some other helpful packages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70871c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f14c6",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "After looking into what our model is doing, we can perform a quantitative evaluation. For this, we need to use the learned model to predict the labels of *unseen* data, in this case our test data `X_test`.\n",
    "\n",
    "Let's first pack everything into a function, since we'll evaluate both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c75ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, model_name, test_data, test_labels):\n",
    "\n",
    "    # Use the learned model to make predictions about unseen data\n",
    "    test_predictions = model.predict(test_data)\n",
    "    \n",
    "    # Print the classification report\n",
    "    print(metrics.classification_report(test_labels, test_predictions, target_names=['low', 'high']))\n",
    "    \n",
    "    # Evaluate by comparing the predictions with the true labels of the test data (here, in a confusion matrix)\n",
    "    confusion_matrix = metrics.confusion_matrix(test_labels,  test_predictions)\n",
    "\n",
    "    # Turn the confusion matrix into a dataframe\n",
    "    matrix_df = pd.DataFrame(confusion_matrix)\n",
    "\n",
    "    # Plot the result\n",
    "    ax = plt.axes()\n",
    "    sns.heatmap(matrix_df, annot=True, fmt=\"g\", ax=ax)\n",
    "    ax.set_title('Confusion Matrix - {}'.format(model_name), fontsize=15)\n",
    "    ax.set_xlabel(\"Predicted Risk_high\", fontsize=15)\n",
    "    ax.set_ylabel(\"Actual Risk_high\", fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf97a3d-4fb3-49c7-a56c-3839da336f3e",
   "metadata": {},
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" width=2000/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232bad1",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## Decision Tree\n",
    "\n",
    "A decision tree is exactly what it sounds like - starting from the top (or the root) of the tree, which contains all data points, we start branching out by choosing feature ranges that allow us to split the data according to the target variable. Having learned a good tree, unseen data is then classified by following the correct tree branches to a tree leaf.\n",
    "\n",
    "<img src=\"images/tree.png\" style=\"margin:auto\" width=500/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2468b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier instance\n",
    "model_dtree = DecisionTreeClassifier(random_state=42, class_weight='balanced')    \n",
    "\n",
    "# Learn from the training data = fit the classifier to the training data\n",
    "model_dtree = model_dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f7bba",
   "metadata": {},
   "source": [
    "#### **Let's visualize the learned tree.** It will be saved in a pdf file in the notebook's directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88192f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_data = export_graphviz(model_dtree, out_file=None, feature_names=features.columns, \n",
    "                                class_names=np.array(['low', 'high']), filled=True, rounded=True, special_characters=True) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"CreditRisk\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e3391-e841-420b-ad3b-73567c0558e1",
   "metadata": {},
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9e0dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_model(model_dtree, 'Decision Tree', X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b730557a-0238-4d52-8495-58bfc68fad7f",
   "metadata": {},
   "source": [
    "\n",
    "The confusion matrix compares the predictions with the true test labels. The diagonal shows the correct guesses and above and below are the 'confusions' where our model was wrong.\n",
    "\n",
    "Generally when doing classification, what we want to see is a diagonal with high numbers. That is not the case here, since the lower right (number of actual high risk customers that were predicted as high risk) is the lowest value. Recognizing the low risk customers, however, seems easy in comparison.\n",
    "\n",
    "If we think about it, in the real world it is much more important to be able to predict high risk customers than low risk customers. As such, this model is far from optimal, due to the higher cost of classifying a high risk loan as low risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a90bb4a",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png\" width=2000/>\n",
    "\n",
    "Let's take look at another classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3676651",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression is a different type of classification algorithm. It used gradient descent to minimize a given error/loss function. It creates a *decision boundary* between positive and negative class samples.\n",
    "\n",
    "<img src=\"images/logreg.png\" style=\"margin:auto\" width=500/>\n",
    "\n",
    "<br>\n",
    "Logistic regression and other gradient methods assume the input data are standardized (to the same mean and standard deviation). \n",
    "Let's scale our data. It's very important that the scaling statistics are calculated only on the training data (otherwise we're cheating!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c77b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create scaler instance\n",
    "scaler = StandardScaler().fit(X_train)  \n",
    "X_train_std = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "X_test_std = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n",
    "\n",
    "X_train_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fd8196-5bf3-44e7-829f-2b8fd2f903af",
   "metadata": {},
   "source": [
    "A result of this transformation is the loss of readability - it's impossible to tell what `Age`=-1.016566 means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a350c462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier instance\n",
    "model_lr = LogisticRegression(class_weight='balanced')    \n",
    "\n",
    "# Learn from the training data = fit the classifier to the training data\n",
    "model_lr = model_lr.fit(X_train_std, y_train)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf2c110-09f4-4e8f-be66-43adf779f7aa",
   "metadata": {},
   "source": [
    "Let's look at the test data results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8335a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_lr, 'Logistic Regression', X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8accce-0c72-4987-b27b-c9409ef9739c",
   "metadata": {},
   "source": [
    "At first glance, performance on the doesn't look much different than the Decision Tree model.\n",
    "\n",
    "However, the recall score for the positive class (`Risk_high`=1) has increased by a good margin, and even the accuracy score (the percentage of correct predictions) has increased slightly.\n",
    "\n",
    "As we've discusses already, for this specific use case what we care about is recognizing high risk customers (`Risk_high` = 1). \n",
    "This means that the worst source of error for any model would be classifying a high risk customer as low risk. This scenario corresponds to the lower left corner of the confusion matrix, and we can see that is the lowest value in the graphic by a good margin. Although the model is not perfect, it can be useful to the people deciding whether to approve a credit or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df05d3f",
   "metadata": {},
   "source": [
    "\n",
    "![green-divider](https://user-images.githubusercontent.com/7065401/52071924-c003ad80-2562-11e9-8297-1c6595f8a7ff.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
